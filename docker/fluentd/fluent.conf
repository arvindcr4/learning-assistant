# Fluentd configuration for Learning Assistant
# Collects logs from Docker containers and forwards to various destinations

# =============================================================================
# Input Sources
# =============================================================================

# Docker container logs
<source>
  @type forward
  port 24224
  bind 0.0.0.0
  tag docker.*
</source>

# Syslog input
<source>
  @type syslog
  port 5140
  bind 0.0.0.0
  tag system.*
</source>

# HTTP input for application logs
<source>
  @type http
  port 8888
  bind 0.0.0.0
  tag app.*
</source>

# File input for local log files
<source>
  @type tail
  path /var/log/learning-assistant/*.log
  pos_file /var/log/fluentd/learning-assistant.log.pos
  tag file.learning-assistant
  format json
  time_format %Y-%m-%d %H:%M:%S
</source>

# =============================================================================
# Filters and Processing
# =============================================================================

# Parse Docker container logs
<filter docker.**>
  @type parser
  key_name log
  reserve_data true
  <parse>
    @type json
    time_format %Y-%m-%dT%H:%M:%S.%L%z
  </parse>
</filter>

# Add container metadata
<filter docker.**>
  @type record_transformer
  <record>
    service ${record["container_name"] || "unknown"}
    environment ${record["environment"] || ENV["NODE_ENV"] || "development"}
    hostname "#{Socket.gethostname}"
    timestamp ${time}
  </record>
</filter>

# Filter sensitive information
<filter **>
  @type grep
  <exclude>
    key message
    pattern /(password|secret|token|key|authorization)/i
  </exclude>
</filter>

# Add correlation ID if missing
<filter **>
  @type record_transformer
  <record>
    correlation_id ${record["correlation_id"] || record["correlationId"] || "unknown"}
  </record>
</filter>

# Classify log levels
<filter **>
  @type record_transformer
  <record>
    log_category ${record["category"] || "general"}
    severity ${record["level"] || record["severity"] || "info"}
  </record>
</filter>

# =============================================================================
# Output Destinations
# =============================================================================

# Development: Output to stdout
<match docker.** system.** app.**>
  @type copy
  
  # Console output for development
  <store>
    @type stdout
    @label @development
  </store>
  
  # File output for local storage
  <store>
    @type file
    path /var/log/fluentd/learning-assistant
    append true
    time_slice_format %Y%m%d
    time_slice_wait 10m
    time_format %Y-%m-%dT%H:%M:%S%z
    format json
    include_time_key true
    
    # Buffer configuration
    <buffer>
      @type file
      path /var/log/fluentd/buffer/learning-assistant
      flush_mode interval
      flush_interval 30s
      chunk_limit_size 10MB
      queue_limit_length 32
      retry_max_interval 30
      retry_forever true
    </buffer>
  </store>
</match>

# Production: Forward to external services
<match docker.** system.** app.**>
  @type copy
  
  # Elasticsearch for log storage and search
  <store>
    @type elasticsearch
    host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
    port "#{ENV['ELASTICSEARCH_PORT'] || 9200}"
    index_name learning-assistant-logs
    type_name _doc
    
    # Template configuration
    template_name learning-assistant-template
    template_file /etc/fluent/templates/elasticsearch.json
    
    # Buffer configuration
    <buffer>
      @type file
      path /var/log/fluentd/buffer/elasticsearch
      flush_mode interval
      flush_interval 10s
      chunk_limit_size 5MB
      queue_limit_length 16
      retry_max_interval 30
      retry_forever true
    </buffer>
    
    # Format configuration
    <format>
      @type json
    </format>
  </store>
  
  # Splunk (if configured)
  <store>
    @type splunk_hec
    protocol https
    hec_host "#{ENV['SPLUNK_HOST']}"
    hec_port "#{ENV['SPLUNK_PORT'] || 8088}"
    hec_token "#{ENV['SPLUNK_TOKEN']}"
    index learning-assistant
    source fluentd
    sourcetype _json
    
    # Only if Splunk is configured
    @include /etc/fluent/conf.d/splunk.conf
  </store>
  
  # DataDog (if configured)
  <store>
    @type datadog
    api_key "#{ENV['DATADOG_API_KEY']}"
    dd_source fluentd
    dd_tags service:learning-assistant,environment:"#{ENV['NODE_ENV']}"
    
    # Only if DataDog is configured
    @include /etc/fluent/conf.d/datadog.conf
  </store>
  
  # S3 for long-term storage
  <store>
    @type s3
    aws_key_id "#{ENV['AWS_ACCESS_KEY_ID']}"
    aws_sec_key "#{ENV['AWS_SECRET_ACCESS_KEY']}"
    s3_bucket "#{ENV['S3_LOG_BUCKET'] || 'learning-assistant-logs'}"
    s3_region "#{ENV['AWS_REGION'] || 'us-west-2'}"
    path logs/
    s3_object_key_format %{path}%{time_slice}_%{index}.%{file_extension}
    time_slice_format %Y/%m/%d/%H
    
    # Buffer configuration
    <buffer time>
      @type file
      path /var/log/fluentd/buffer/s3
      timekey 3600
      timekey_wait 10m
      chunk_limit_size 256MB
    </buffer>
    
    # Format configuration
    <format>
      @type json
    </format>
    
    # Only if S3 is configured
    @include /etc/fluent/conf.d/s3.conf
  </store>
</match>

# Error logs - high priority routing
<match docker.**error** app.**error**>
  @type copy
  
  # Immediate notification for critical errors
  <store>
    @type slack
    webhook_url "#{ENV['SLACK_WEBHOOK_URL']}"
    channel "#alerts"
    username "Learning Assistant Logs"
    icon_emoji ":warning:"
    title "Critical Error Detected"
    message "Error: %{message}\nService: %{service}\nTime: %{time}"
    
    # Only for error level and above
    <filter>
      key severity
      pattern /^(error|fatal|critical)$/
    </filter>
  </store>
  
  # Email notification for critical errors
  <store>
    @type mail
    host "#{ENV['SMTP_HOST']}"
    port "#{ENV['SMTP_PORT'] || 587}"
    user "#{ENV['SMTP_USER']}"
    password "#{ENV['SMTP_PASSWORD']}"
    from "alerts@learningassistant.com"
    to "#{ENV['ALERT_EMAIL'] || 'admin@learningassistant.com'}"
    subject "Learning Assistant Critical Error"
    
    # Only for critical errors
    <filter>
      key severity
      pattern /^(fatal|critical)$/
    </filter>
  </store>
</match>

# Security logs - special handling
<match **security** **auth**>
  @type copy
  
  # Security SIEM system
  <store>
    @type syslog
    host "#{ENV['SIEM_HOST']}"
    port "#{ENV['SIEM_PORT'] || 514}"
    facility local0
    severity info
    tag learning-assistant-security
  </store>
  
  # Separate security log file
  <store>
    @type file
    path /var/log/fluentd/security/learning-assistant-security
    append true
    time_slice_format %Y%m%d
    time_slice_wait 10m
    format json
    include_time_key true
    
    <buffer>
      @type file
      path /var/log/fluentd/buffer/security
      flush_mode interval
      flush_interval 10s
      chunk_limit_size 5MB
    </buffer>
  </store>
</match>

# =============================================================================
# Labels and Conditional Processing
# =============================================================================

<label @development>
  <match **>
    @type stdout
    <format>
      @type json
    </format>
  </match>
</label>

# =============================================================================
# System Configuration
# =============================================================================

# Include additional configuration files
@include /etc/fluent/conf.d/*.conf

# Log Fluentd's own logs
<system>
  log_level info
  suppress_repeated_stacktrace true
  emit_error_log_interval 30s
  suppress_config_dump true
</system>